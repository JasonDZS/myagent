# Traceç³»ç»Ÿä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»å¦‚ä½•åœ¨MyAgentæ¡†æ¶ä¸­ä½¿ç”¨traceç³»ç»Ÿè¿›è¡Œè°ƒè¯•ã€ç›‘æ§å’Œåˆ†æã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å¯ç”¨TraceåŠŸèƒ½

```python
from myagent import create_react_agent
from myagent.tool import Terminate

# åˆ›å»ºæ”¯æŒtraceçš„ä»£ç†
agent = create_react_agent(
    name="demo_agent",
    tools=[Terminate()],
    enable_tracing=True  # å¯ç”¨traceåŠŸèƒ½
)

# æ‰§è¡Œä»»åŠ¡ï¼Œè‡ªåŠ¨è®°å½•trace
result = await agent.run("ä½ çš„è¯·æ±‚")
```

### å¯¼å‡ºTraceæ•°æ®

```python
from myagent.trace import TraceQueryEngine, TraceExporter, get_trace_manager

async def export_traces():
    # è·å–traceç®¡ç†å™¨
    trace_manager = get_trace_manager()
    query_engine = TraceQueryEngine(trace_manager.storage)
    exporter = TraceExporter(query_engine)

    # å¯¼å‡ºä¸ºJSONæ ¼å¼
    json_data = await exporter.export_traces_to_json()

    # ä¿å­˜åˆ°æ–‡ä»¶
    with open("traces/my_trace.json", "w", encoding="utf-8") as f:
        f.write(json_data)

    # å¯¼å‡ºç»Ÿè®¡æŠ¥å‘Š
    summary = await exporter.export_trace_summary()
    print(summary)
```

## ğŸ“Š Traceæ•°æ®åˆ†æ

### åŸºç¡€æ•°æ®ç»“æ„åˆ†æ

```python
import json

def analyze_trace_structure(trace_file):
    """åˆ†ætraceæ–‡ä»¶çš„åŸºç¡€ç»“æ„"""
    with open(trace_file, 'r', encoding='utf-8') as f:
        trace_data = json.load(f)

    # æå–æ‰€æœ‰runs
    runs = []
    for trace in trace_data.get('traces', []):
        runs.extend(trace.get('runs', []))

    # æŒ‰ç±»å‹åˆ†ç»„
    runs_by_type = {}
    for run in runs:
        run_type = run.get('run_type')
        if run_type not in runs_by_type:
            runs_by_type[run_type] = []
        runs_by_type[run_type].append(run)

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("=== Trace Structure Analysis ===")
    for run_type, run_list in runs_by_type.items():
        print(f"{run_type.upper()} runs: {len(run_list)}")

    return runs_by_type

# ä½¿ç”¨ç¤ºä¾‹
runs_by_type = analyze_trace_structure("traces/my_trace.json")
```

### æ¨ç†è¿‡ç¨‹åˆ†æ

```python
def analyze_thinking_process(runs_by_type):
    """åˆ†æAgentçš„æ¨ç†è¿‡ç¨‹"""
    think_runs = runs_by_type.get('think', [])

    print("\\n=== Thinking Process Analysis ===")
    for i, run in enumerate(think_runs, 1):
        print(f"\\n--- Step {i} Think ---")

        # è¾“å…¥åˆ†æ
        inputs = run.get('inputs', {})
        user_content = inputs.get('content', '')
        print(f"User Input: {user_content[:100]}...")

        # è¾“å‡ºåˆ†æ
        outputs = run.get('outputs', {})
        assistant_content = outputs.get('content', '')
        tool_calls = outputs.get('tool_calls', [])

        print(f"Assistant Response: {assistant_content[:100]}...")
        print(f"Tools Planned: {[call['function']['name'] for call in tool_calls]}")
        print(f"Think Time: {run.get('latency_ms', 0):.2f}ms")

# ä½¿ç”¨ç¤ºä¾‹
analyze_thinking_process(runs_by_type)
```

### å·¥å…·ä½¿ç”¨åˆ†æ

```python
def analyze_tool_usage(runs_by_type):
    """åˆ†æå·¥å…·ä½¿ç”¨æƒ…å†µ"""
    tool_runs = runs_by_type.get('tool', [])

    # å·¥å…·ä½¿ç”¨é¢‘ç‡ç»Ÿè®¡
    tool_frequency = {}
    tool_performance = {}

    for run in tool_runs:
        tool_name = run.get('name')
        latency = run.get('latency_ms', 0)
        status = run.get('status', 'unknown')

        # é¢‘ç‡ç»Ÿè®¡
        if tool_name not in tool_frequency:
            tool_frequency[tool_name] = {'success': 0, 'error': 0, 'total': 0}

        tool_frequency[tool_name]['total'] += 1
        if status == 'success':
            tool_frequency[tool_name]['success'] += 1
        else:
            tool_frequency[tool_name]['error'] += 1

        # æ€§èƒ½ç»Ÿè®¡
        if tool_name not in tool_performance:
            tool_performance[tool_name] = []
        tool_performance[tool_name].append(latency)

    print("\\n=== Tool Usage Analysis ===")
    for tool_name, freq in tool_frequency.items():
        success_rate = freq['success'] / freq['total'] * 100
        avg_latency = sum(tool_performance[tool_name]) / len(tool_performance[tool_name])

        print(f"\\n{tool_name}:")
        print(f"  Total Calls: {freq['total']}")
        print(f"  Success Rate: {success_rate:.1f}%")
        print(f"  Average Latency: {avg_latency:.2f}ms")

# ä½¿ç”¨ç¤ºä¾‹
analyze_tool_usage(runs_by_type)
```

### æ€§èƒ½åˆ†æ

```python
def analyze_performance(runs_by_type):
    """åˆ†ææ‰§è¡Œæ€§èƒ½"""
    agent_runs = runs_by_type.get('agent', [])
    think_runs = runs_by_type.get('think', [])
    tool_runs = runs_by_type.get('tool', [])

    print("\\n=== Performance Analysis ===")

    # Agentæ­¥éª¤åˆ†æ
    if agent_runs:
        step_times = [run.get('latency_ms', 0) for run in agent_runs]
        print(f"\\nAgent Steps:")
        print(f"  Total Steps: {len(step_times)}")
        print(f"  Total Time: {sum(step_times):.2f}ms")
        print(f"  Average Step Time: {sum(step_times)/len(step_times):.2f}ms")

    # æ¨ç†æ—¶é—´åˆ†æ
    if think_runs:
        think_times = [run.get('latency_ms', 0) for run in think_runs]
        print(f"\\nThinking Performance:")
        print(f"  Total Think Time: {sum(think_times):.2f}ms")
        print(f"  Average Think Time: {sum(think_times)/len(think_times):.2f}ms")
        print(f"  Min/Max Think Time: {min(think_times):.2f}ms / {max(think_times):.2f}ms")

    # å·¥å…·æ‰§è¡Œæ—¶é—´åˆ†æ
    if tool_runs:
        tool_times = [run.get('latency_ms', 0) for run in tool_runs]
        print(f"\\nTool Execution Performance:")
        print(f"  Total Tool Time: {sum(tool_times):.2f}ms")
        print(f"  Average Tool Time: {sum(tool_times)/len(tool_times):.2f}ms")

        # æœ€æ…¢çš„å·¥å…·
        slowest_tool = max(tool_runs, key=lambda x: x.get('latency_ms', 0))
        print(f"  Slowest Tool: {slowest_tool.get('name')} ({slowest_tool.get('latency_ms', 0):.2f}ms)")

# ä½¿ç”¨ç¤ºä¾‹
analyze_performance(runs_by_type)
```

## ğŸ› è°ƒè¯•åœºæ™¯

### æŸ¥æ‰¾æ¨ç†é”™è¯¯

```python
def debug_thinking_issues(runs_by_type):
    """è°ƒè¯•æ¨ç†ç›¸å…³é—®é¢˜"""
    think_runs = runs_by_type.get('think', [])

    print("\\n=== Thinking Issues Debug ===")

    for i, run in enumerate(think_runs, 1):
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if run.get('status') != 'success':
            print(f"\\nâŒ Think Step {i} Error:")
            print(f"  Error: {run.get('error')}")
            print(f"  Error Type: {run.get('error_type')}")

        # æ£€æŸ¥å¼‚å¸¸é•¿çš„æ¨ç†æ—¶é—´
        latency = run.get('latency_ms', 0)
        if latency > 10000:  # è¶…è¿‡10ç§’
            print(f"\\nâš ï¸  Think Step {i} Slow Performance:")
            print(f"  Latency: {latency:.2f}ms")
            print(f"  Input Length: {len(run.get('inputs', {}).get('content', ''))}")

        # æ£€æŸ¥æ— å·¥å…·è°ƒç”¨çš„æƒ…å†µ
        outputs = run.get('outputs', {})
        tool_calls = outputs.get('tool_calls', [])
        if not tool_calls and 'terminate' not in outputs.get('content', '').lower():
            print(f"\\nğŸ¤” Think Step {i} No Tool Calls:")
            print(f"  Response: {outputs.get('content', '')[:200]}...")

# ä½¿ç”¨ç¤ºä¾‹
debug_thinking_issues(runs_by_type)
```

### æŸ¥æ‰¾å·¥å…·æ‰§è¡Œé”™è¯¯

```python
def debug_tool_issues(runs_by_type):
    """è°ƒè¯•å·¥å…·æ‰§è¡Œé—®é¢˜"""
    tool_runs = runs_by_type.get('tool', [])

    print("\\n=== Tool Issues Debug ===")

    error_count = 0
    for run in tool_runs:
        if run.get('status') != 'success':
            error_count += 1
            print(f"\\nâŒ Tool Error #{error_count}:")
            print(f"  Tool: {run.get('name')}")
            print(f"  Error: {run.get('error')}")
            print(f"  Inputs: {run.get('inputs', {})}")

            # æŸ¥çœ‹ç›¸å…³çš„Thinkå†³ç­–
            parent_id = run.get('parent_run_id')
            print(f"  Parent Step ID: {parent_id}")

    # æŸ¥æ‰¾æ‰§è¡Œæ—¶é—´å¼‚å¸¸çš„å·¥å…·
    slow_tools = []
    for run in tool_runs:
        latency = run.get('latency_ms', 0)
        if latency > 5000:  # è¶…è¿‡5ç§’
            slow_tools.append((run.get('name'), latency))

    if slow_tools:
        print(f"\\nâš ï¸  Slow Tools:")
        for tool_name, latency in sorted(slow_tools, key=lambda x: x[1], reverse=True):
            print(f"  {tool_name}: {latency:.2f}ms")

# ä½¿ç”¨ç¤ºä¾‹
debug_tool_issues(runs_by_type)
```

## ğŸ“ˆ ç›‘æ§å’Œå‘Šè­¦

### å®æ—¶ç›‘æ§æŒ‡æ ‡

```python
class TraceMonitor:
    """Traceç›‘æ§ç±»"""

    def __init__(self, thresholds=None):
        self.thresholds = thresholds or {
            'max_think_time': 10000,  # 10ç§’
            'max_tool_time': 5000,    # 5ç§’
            'max_step_time': 30000,   # 30ç§’
            'min_success_rate': 0.95  # 95%
        }

    async def monitor_trace(self, trace_data):
        """ç›‘æ§traceæ•°æ®å¹¶ç”Ÿæˆå‘Šè­¦"""
        alerts = []

        runs = []
        for trace in trace_data.get('traces', []):
            runs.extend(trace.get('runs', []))

        # æŒ‰ç±»å‹åˆ†ç»„
        runs_by_type = {}
        for run in runs:
            run_type = run.get('run_type')
            if run_type not in runs_by_type:
                runs_by_type[run_type] = []
            runs_by_type[run_type].append(run)

        # æ£€æŸ¥æ¨ç†æ—¶é—´
        think_runs = runs_by_type.get('think', [])
        for run in think_runs:
            if run.get('latency_ms', 0) > self.thresholds['max_think_time']:
                alerts.append({
                    'type': 'SLOW_THINKING',
                    'run_id': run.get('id'),
                    'latency': run.get('latency_ms'),
                    'threshold': self.thresholds['max_think_time']
                })

        # æ£€æŸ¥å·¥å…·æ‰§è¡Œæ—¶é—´å’ŒæˆåŠŸç‡
        tool_runs = runs_by_type.get('tool', [])
        tool_stats = {}

        for run in tool_runs:
            tool_name = run.get('name')
            if tool_name not in tool_stats:
                tool_stats[tool_name] = {'total': 0, 'success': 0, 'latencies': []}

            tool_stats[tool_name]['total'] += 1
            if run.get('status') == 'success':
                tool_stats[tool_name]['success'] += 1
            tool_stats[tool_name]['latencies'].append(run.get('latency_ms', 0))

        # ç”Ÿæˆå·¥å…·ç›¸å…³å‘Šè­¦
        for tool_name, stats in tool_stats.items():
            # æˆåŠŸç‡å‘Šè­¦
            success_rate = stats['success'] / stats['total']
            if success_rate < self.thresholds['min_success_rate']:
                alerts.append({
                    'type': 'LOW_SUCCESS_RATE',
                    'tool': tool_name,
                    'success_rate': success_rate,
                    'threshold': self.thresholds['min_success_rate']
                })

            # æ€§èƒ½å‘Šè­¦
            avg_latency = sum(stats['latencies']) / len(stats['latencies'])
            if avg_latency > self.thresholds['max_tool_time']:
                alerts.append({
                    'type': 'SLOW_TOOL',
                    'tool': tool_name,
                    'avg_latency': avg_latency,
                    'threshold': self.thresholds['max_tool_time']
                })

        return alerts

    def format_alerts(self, alerts):
        """æ ¼å¼åŒ–å‘Šè­¦ä¿¡æ¯"""
        if not alerts:
            return "âœ… No alerts - all metrics within thresholds"

        result = f"ğŸš¨ {len(alerts)} Alert(s) Generated:\\n\\n"

        for alert in alerts:
            if alert['type'] == 'SLOW_THINKING':
                result += f"âš ï¸  Slow Thinking: {alert['latency']:.0f}ms (>{alert['threshold']}ms)\\n"
            elif alert['type'] == 'LOW_SUCCESS_RATE':
                result += f"âŒ Low Success Rate: {alert['tool']} = {alert['success_rate']:.1%} (<{alert['threshold']:.1%})\\n"
            elif alert['type'] == 'SLOW_TOOL':
                result += f"ğŸŒ Slow Tool: {alert['tool']} = {alert['avg_latency']:.0f}ms (>{alert['threshold']}ms)\\n"

        return result

# ä½¿ç”¨ç¤ºä¾‹
async def run_monitoring():
    # å¯¼å‡ºæœ€æ–°traceæ•°æ®
    trace_manager = get_trace_manager()
    query_engine = TraceQueryEngine(trace_manager.storage)
    exporter = TraceExporter(query_engine)

    json_data = await exporter.export_traces_to_json()
    trace_data = json.loads(json_data)

    # ç›‘æ§å¹¶ç”Ÿæˆå‘Šè­¦
    monitor = TraceMonitor()
    alerts = await monitor.monitor_trace(trace_data)

    # è¾“å‡ºå‘Šè­¦ä¿¡æ¯
    print(monitor.format_alerts(alerts))
```

## ğŸ”§ è‡ªå®šä¹‰å·¥å…·çš„Traceæ”¯æŒ

### åŸºç¡€å·¥å…·Trace

```python
from myagent.tool import BaseTool, ToolResult

class DatabaseQueryTool(BaseTool):
    name = "db_query"
    description = "Execute database queries"
    parameters = {
        "type": "object",
        "properties": {
            "sql": {"type": "string", "description": "SQL query to execute"},
            "limit": {"type": "integer", "description": "Result limit", "default": 100}
        },
        "required": ["sql"]
    }

    async def execute(self, sql: str, limit: int = 100) -> ToolResult:
        """æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢

        æ³¨æ„ï¼šæ‰€æœ‰å‚æ•°å’Œè¿”å›å€¼éƒ½ä¼šè‡ªåŠ¨è®°å½•åˆ°traceä¸­
        - inputs: {"sql": "SELECT...", "limit": 100}
        - outputs: {"output": "æŸ¥è¯¢ç»“æœ", "error": null}
        """
        try:
            # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
            result = f"Query executed: {sql[:50]}... (limit: {limit})"
            return ToolResult(output=result)

        except Exception as e:
            return ToolResult(error=str(e))
```

### ç¦ç”¨ç‰¹å®šå·¥å…·çš„Trace

```python
class SensitiveTool(BaseTool):
    name = "sensitive_operation"
    description = "Perform sensitive operations"
    enable_tracing = False  # ç¦ç”¨traceè®°å½•

    async def execute(self, secret_param: str) -> str:
        # æ•æ„Ÿæ“ä½œï¼Œä¸ä¼šè®°å½•åˆ°traceä¸­
        return "Operation completed"
```

### è‡ªå®šä¹‰Traceå…ƒæ•°æ®

```python
class AdvancedTool(BaseTool):
    name = "advanced_tool"
    description = "Tool with custom trace metadata"

    async def execute(self, data: dict) -> ToolResult:
        # å·¥å…·æ‰§è¡Œé€»è¾‘
        result = {"processed": len(data)}

        # è¿”å›ç»“æœæ—¶å¯ä»¥æ·»åŠ ç³»ç»Ÿä¿¡æ¯
        return ToolResult(
            output=result,
            system=f"Processed {len(data)} items"  # ä¼šè®°å½•åˆ°traceä¸­
        )
```

## ğŸ“‹ æœ€ä½³å®è·µ

### 1. Traceå­˜å‚¨ç®¡ç†

```python
# å®šæœŸæ¸…ç†æ—§çš„traceæ•°æ®
import os
from datetime import datetime, timedelta

def cleanup_old_traces(trace_dir="workdir/traces", days_to_keep=7):
    """æ¸…ç†æŒ‡å®šå¤©æ•°ä¹‹å‰çš„traceæ–‡ä»¶"""
    cutoff_date = datetime.now() - timedelta(days=days_to_keep)

    for filename in os.listdir(trace_dir):
        if filename.endswith('.json'):
            file_path = os.path.join(trace_dir, filename)
            file_time = datetime.fromtimestamp(os.path.getmtime(file_path))

            if file_time < cutoff_date:
                os.remove(file_path)
                print(f"Deleted old trace: {filename}")
```

### 2. ç”Ÿäº§ç¯å¢ƒé…ç½®

```python
# ç”Ÿäº§ç¯å¢ƒå»ºè®®é…ç½®
agent = create_react_agent(
    name="production_agent",
    tools=tools,
    enable_tracing=True,  # ä¿æŒå¼€å¯ä»¥ä¾¿ç›‘æ§
    trace_metadata=TraceMetadata(
        environment="production",
        version="1.0.0",
        custom_fields={
            "user_id": "user123",
            "session_id": "session456"
        }
    )
)
```

### 3. å¼€å‘ç¯å¢ƒè°ƒè¯•

```python
# å¼€å‘ç¯å¢ƒè¯¦ç»†è°ƒè¯•é…ç½®
agent = create_react_agent(
    name="debug_agent",
    tools=tools,
    enable_tracing=True,
    max_steps=5,  # é™åˆ¶æ­¥éª¤æ•°ä¾¿äºè°ƒè¯•
    trace_metadata=TraceMetadata(
        environment="development",
        debug_mode=True
    )
)

# æ‰§è¡Œåç«‹å³åˆ†æ
result = await agent.run("æµ‹è¯•è¯·æ±‚")
trace_data = await export_and_analyze_traces()
```

é€šè¿‡è¿™äº›å·¥å…·å’Œæ–¹æ³•ï¼Œä½ å¯ä»¥å……åˆ†åˆ©ç”¨MyAgentçš„traceç³»ç»Ÿæ¥ç›‘æ§ã€è°ƒè¯•å’Œä¼˜åŒ–ä½ çš„AIä»£ç†åº”ç”¨ã€‚
